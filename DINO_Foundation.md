# DINO: Self-Distillation with NO Labels
## TÃ i liá»‡u tham kháº£o toÃ n diá»‡n

> **Äá»‘i tÆ°á»£ng**: CS students cÃ³ kiáº¿n thá»©c ML cÆ¡ báº£n (neural network, loss function, gradient descent)
> **PhÆ°Æ¡ng phÃ¡p**: Examples first, theory second

---

# Part 0: DINO LÃ m ÄÆ°á»£c GÃ¬?

## 0.1 Demo: Attention Maps Tá»± Segment Váº­t Thá»ƒ

TrÆ°á»›c khi Ä‘i vÃ o lÃ½ thuyáº¿t, hÃ£y xem DINO lÃ m Ä‘Æ°á»£c gÃ¬.

**VÃ­ dá»¥ 1: Attention map trÃªn áº£nh con chÃ³**
```
[INPUT]                    [ATTENTION MAPS]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚           â”‚ Head 1  â”‚ â”‚ Head 2  â”‚ â”‚ Head 3  â”‚
â”‚   ğŸ• Dog    â”‚    â†’      â”‚  Äáº§u    â”‚ â”‚  ThÃ¢n   â”‚ â”‚  Ná»n    â”‚
â”‚             â”‚           â”‚ (sÃ¡ng)  â”‚ â”‚ (sÃ¡ng)  â”‚ â”‚ (sÃ¡ng)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

KhÃ´ng ai dáº¡y model "Ä‘Ã¢y lÃ  Ä‘áº§u", "Ä‘Ã¢y lÃ  thÃ¢n" â€” nÃ³ tá»± há»c phÃ¢n biá»‡t!

**VÃ­ dá»¥ 2: Semantic segmentation khÃ´ng cáº§n annotation**
- Input: áº¢nh xe hÆ¡i trÃªn Ä‘Æ°á»ng
- Output: Tá»± Ä‘á»™ng tÃ¡ch xe, Ä‘Æ°á»ng, báº§u trá»i thÃ nh cÃ¡c vÃ¹ng riÃªng biá»‡t
- KhÃ´ng cáº§n 1 pixel annotation nÃ o

## 0.2 So SÃ¡nh Chi PhÃ­

| PhÆ°Æ¡ng phÃ¡p | Labels cáº§n | Chi phÃ­ Æ°á»›c tÃ­nh | Thá»i gian |
|-------------|------------|------------------|-----------|
| **ImageNet Supervised** | 14M áº£nh Ã— 3 ngÆ°á»i kiá»ƒm tra | $500K+ | 2 nÄƒm |
| **OpenCLIP** | 400M text-image pairs | Cáº§n crawl + filter | ThÃ¡ng |
| **DINO** | 0 | Chá»‰ cáº§n áº£nh raw | Tuáº§n |

## 0.3 TÃ³m Táº¯t 1 PhÃºt

DINO = **D**istillation with **NO** labels

**Ã tÆ°á»Ÿng cá»‘t lÃµi**:
1. Táº¡o 2 phiÃªn báº£n cá»§a cÃ¹ng 1 model: Teacher vÃ  Student
2. Teacher nhÃ¬n toÃ n cáº£nh, Student nhÃ¬n cá»¥c bá»™
3. Student pháº£i Ä‘oÃ¡n output giá»‘ng Teacher
4. KhÃ´ng cáº§n nhÃ£n â€” Teacher chÃ­nh lÃ  "Ä‘Ã¡p Ã¡n"

**Káº¿t quáº£**: 88.4% ImageNet accuracy, vÆ°á»£t supervised learning (85.7%)

---

# Part 1: Prerequisites

## 1.1 Tá»« áº¢nh Äáº¿n Patches

### VÃ­ dá»¥ trÆ°á»›c

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n cÃ³ má»™t bá»©c áº£nh 224Ã—224 pixels. Thay vÃ¬ xá»­ lÃ½ tá»«ng pixel má»™t (224Ã—224 = 50,176 pixels â€” quÃ¡ nhiá»u!), ta "cáº¯t" áº£nh thÃ nh cÃ¡c máº£nh nhá» hÆ¡n.

```
áº¢nh gá»‘c 224Ã—224
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€...â”€â”¬â”€â”€â”€â”€â”
â”‚ P1 â”‚ P2 â”‚ P3 â”‚     â”‚P14 â”‚  â† HÃ ng 1: 14 patches
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€...â”€â”¼â”€â”€â”€â”€â”¤
â”‚P15 â”‚P16 â”‚P17 â”‚     â”‚P28 â”‚  â† HÃ ng 2: 14 patches
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€...â”€â”¼â”€â”€â”€â”€â”¤
â”‚... â”‚    â”‚    â”‚     â”‚... â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€...â”€â”¼â”€â”€â”€â”€â”¤
â”‚    â”‚    â”‚    â”‚     â”‚P196â”‚  â† HÃ ng 14: 14 patches
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€...â”€â”´â”€â”€â”€â”€â”˜

Má»—i patch: 16Ã—16 pixels
Tá»•ng: 14Ã—14 = 196 patches
```

### Táº¡i sao 16Ã—16?

- QuÃ¡ nhá» (4Ã—4): 3,136 patches â†’ sequence quÃ¡ dÃ i
- QuÃ¡ lá»›n (32Ã—32): 49 patches â†’ máº¥t chi tiáº¿t
- 16Ã—16: Balance giá»¯a chi tiáº¿t vÃ  Ä‘á»™ dÃ i sequence

### Tá»« Patch Äáº¿n Vector

Má»—i patch 16Ã—16Ã—3 (RGB) = 768 sá»‘ â†’ **Linear projection** â†’ Vector 768 chiá»u

```python
# Pseudocode
patch = image[0:16, 0:16, :]  # 16Ã—16Ã—3 = 768 numbers
embedding = linear_layer(patch.flatten())  # â†’ 768-dim vector
```

**Thuáº­t ngá»¯**:
- **Patch**: Má»™t máº£nh nhá» cá»§a áº£nh (16Ã—16 pixels)
- **Embedding**: Vector sá»‘ Ä‘áº¡i diá»‡n cho patch
- **Positional encoding**: Cho model biáº¿t patch á»Ÿ vá»‹ trÃ­ nÃ o (thÃªm vÃ o embedding)

## 1.2 Attention Mechanism

### VÃ­ dá»¥: TÃ¬m thÃ´ng tin trong thÆ° viá»‡n

Báº¡n muá»‘n tÃ¬m sÃ¡ch vá» "machine learning". QuÃ¡ trÃ¬nh:

1. **Query (Q)**: CÃ¢u há»i cá»§a báº¡n â€” "machine learning"
2. **Key (K)**: TiÃªu Ä‘á»/tag cá»§a má»—i cuá»‘n sÃ¡ch
3. **Value (V)**: Ná»™i dung thá»±c sá»± cá»§a má»—i cuá»‘n sÃ¡ch

```
Báº¡n há»i: "machine learning" (Query)
                    â†“
         So sÃ¡nh vá»›i má»—i Key
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SÃ¡ch 1: "Deep Learning" â†’ Match cao    â”‚ â†’ Láº¥y nhiá»u ná»™i dung
â”‚ SÃ¡ch 2: "Statistics"    â†’ Match vá»«a    â”‚ â†’ Láº¥y Ã­t ná»™i dung
â”‚ SÃ¡ch 3: "Cooking"       â†’ Match tháº¥p   â”‚ â†’ Gáº§n nhÆ° bá» qua
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         Tá»•ng há»£p ná»™i dung (Values)
                    â†“
              CÃ¢u tráº£ lá»i
```

### CÃ´ng thá»©c Attention

```
Attention(Q, K, V) = softmax(QK^T / âˆšd) Ã— V
```

Giáº£i thÃ­ch tá»«ng pháº§n:

| Pháº§n | Ã nghÄ©a | VÃ­ dá»¥ |
|------|---------|-------|
| `QK^T` | Äo Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a Q vÃ  K | "machine learning" khá»›p vá»›i "Deep Learning" bao nhiÃªu? |
| `âˆšd` | Chia Ä‘á»ƒ á»•n Ä‘á»‹nh (d=64 trong ViT-B) | KhÃ´ng cho sá»‘ quÃ¡ lá»›n trÆ°á»›c softmax |
| `softmax` | Chuyá»ƒn thÃ nh xÃ¡c suáº¥t (tá»•ng = 1) | [0.7, 0.2, 0.1] thay vÃ¬ [5.2, 1.3, 0.5] |
| `Ã— V` | Láº¥y ná»™i dung theo trá»ng sá»‘ | 70% tá»« sÃ¡ch 1, 20% tá»« sÃ¡ch 2, 10% tá»« sÃ¡ch 3 |

### Multi-Head Attention

Thay vÃ¬ 1 gÃ³c nhÃ¬n, dÃ¹ng nhiá»u gÃ³c nhÃ¬n (heads) cÃ¹ng lÃºc:

```
Head 1: Focus vÃ o texture (lÃ´ng, váº£y)
Head 2: Focus vÃ o shape (hÃ¬nh dÃ¡ng tá»•ng thá»ƒ)
Head 3: Focus vÃ o color (mÃ u sáº¯c)
...
Head 12: Focus vÃ o context (background)
```

ViT-B: 12 heads Ã— 64 chiá»u/head = 768 chiá»u tá»•ng

## 1.3 CLS Token

### Váº¥n Ä‘á»

Sau khi xá»­ lÃ½ 196 patches qua Transformer, ta cÃ³ 196 output vectors. NhÆ°ng ta cáº§n **1 vector duy nháº¥t** Ä‘áº¡i diá»‡n cho toÃ n bá»™ áº£nh.

### Giáº£i phÃ¡p: CLS Token

ThÃªm 1 token Ä‘áº·c biá»‡t á»Ÿ Ä‘áº§u sequence:

```
Input:  [CLS] [P1] [P2] [P3] ... [P196]
         â†“
    Transformer (12 layers)
         â†“
Output: [CLS'] [P1'] [P2'] [P3'] ... [P196']
         â†‘
    Láº¥y vector nÃ y lÃ m Ä‘áº¡i diá»‡n
```

**Táº¡i sao CLS hoáº¡t Ä‘á»™ng?**

CLS khÃ´ng gáº¯n vá»›i pixel nÃ o. Qua attention, nÃ³ "láº¯ng nghe" táº¥t cáº£ 196 patches vÃ  tá»± há»c cÃ¡ch tá»•ng há»£p thÃ´ng tin quan trá»ng nháº¥t.

```
CLS: "TÃ´i khÃ´ng biáº¿t gÃ¬, nhÆ°ng tÃ´i sáº½ há»i táº¥t cáº£ cÃ¡c patches"
     â†“
Layer 1: CLS attention Ä‘áº¿n P1, P2, P3...
Layer 2: CLS tinh chá»‰nh dá»±a trÃªn layer 1
...
Layer 12: CLS Ä‘Ã£ "hiá»ƒu" toÃ n bá»™ áº£nh
```

**Trong DINO**: Output CLS cá»§a Teacher vÃ  Student Ä‘Æ°á»£c so sÃ¡nh vá»›i nhau.

## 1.4 Knowledge Distillation

### Ã tÆ°á»Ÿng tá»« Hinton (2015)

Model lá»›n Ä‘Ã£ train xong (Teacher) â†’ "dáº¡y" cho model nhá» (Student)

**Táº¡i sao khÃ´ng chá»‰ dÃ¹ng nhÃ£n cá»©ng?**

```
NhÃ£n cá»©ng:  [1, 0, 0]  = "ÄÃ¢y lÃ  mÃ¨o"
NhÃ£n má»m:   [0.8, 0.15, 0.05] = "80% mÃ¨o, 15% chÃ³, 5% há»•"
```

NhÃ£n má»m chá»©a thÃ´ng tin há»¯u Ã­ch: "MÃ¨o giá»‘ng chÃ³ hÆ¡n giá»‘ng Ã´ tÃ´"

### Self-Distillation trong DINO

DINO Ä‘áº·c biá»‡t: Teacher vÃ  Student **cÃ¹ng kiáº¿n trÃºc, cÃ¹ng kÃ­ch thÆ°á»›c**. Teacher khÃ´ng train sáºµn mÃ  Ä‘Æ°á»£c táº¡o tá»« Student qua EMA.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Student   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ EMA (copy cháº­m)
                           â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Teacher   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 2: DINOv1 (2021)

## 2.1 Core Insight: Local â†’ Global

### VÃ­ dá»¥

Báº¡n tháº¥y má»™t cÃ¡i vÃ¢y cÃ¡. Báº¡n biáº¿t Ä‘Ã³ lÃ  con cÃ¡, dÃ¹ chá»‰ tháº¥y má»™t pháº§n nhá».

DINO Ã¡p dá»¥ng Ã½ tÆ°á»Ÿng nÃ y:
- **Teacher** nhÃ¬n toÃ n bá»™ áº£nh (global crop, >50% áº£nh)
- **Student** nhÃ¬n má»™t gÃ³c nhá» (local crop, <50% áº£nh)
- **Má»¥c tiÃªu**: Student pháº£i output giá»‘ng Teacher

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   â”‚     â”‚ Local   â”‚
â”‚   Global crop     â”‚     â”‚ crop    â”‚
â”‚   (Teacher nhÃ¬n)  â”‚     â”‚(Student)â”‚
â”‚                   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                      â†“
   Output: P_t              Output: P_s
        â†“                      â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€ Loss â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           P_s pháº£i â‰ˆ P_t
```

**Táº¡i sao Ä‘iá»u nÃ y hoáº¡t Ä‘á»™ng?**

Student chá»‰ tháº¥y cÃ¡i vÃ¢y, nhÆ°ng pháº£i Ä‘oÃ¡n "Ä‘Ã¢y lÃ  cÃ¡" (giá»‘ng Teacher Ä‘ang tháº¥y cáº£ con cÃ¡). Buá»™c Student pháº£i **hiá»ƒu ngá»¯ cáº£nh**, khÃ´ng chá»‰ copy pixel.

## 2.2 Kiáº¿n TrÃºc DINO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DINO Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  áº¢nh gá»‘c                                                     â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€â”€â†’ Global crops (2 cÃ¡i, 224Ã—224) â”€â”€â†’ Teacher ViT       â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                                     Projection Head     â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                                     softmax(Ï„=0.04)     â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                                         P_t             â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                      â–¼                                  â”‚
â”‚    â”‚              Loss = -Î£ P_t Â· log(P_s)                   â”‚
â”‚    â”‚                      â–²                                  â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                     P_s                                 â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                 softmax(Ï„=0.1)                          â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                Projection Head                          â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â””â”€â”€â†’ Local crops (6 cÃ¡i, 96Ã—96) â”€â”€â”€â†’ Student ViT         â”‚
â”‚                                                              â”‚
â”‚    [EMA: Î¸_T â† 0.996Â·Î¸_T + 0.004Â·Î¸_S]                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Kiáº¿n trÃºc chi tiáº¿t

**Network Structure**: `g = h âˆ˜ f` (backbone f + projection head h)

```
Input Image â†’ ViT Backbone (f) â†’ CLS token â†’ Projection Head (h) â†’ Output
                 â”‚                   â”‚              â”‚
                 â”‚                   â”‚              â””â”€â†’ 3-layer MLP
                 â”‚                   â”‚                  2048 hidden dim
                 â”‚                   â”‚                  K output dims
                 â”‚                   â”‚                  Weight normalized
                 â”‚                   â”‚                  L2 normalized
                 â”‚                   â”‚
                 â”‚                   â””â”€â†’ 768-dim (ViT-B)
                 â”‚
                 â””â”€â†’ 12 Transformer blocks
                     12 attention heads
                     **HoÃ n toÃ n KHÃ”NG cÃ³ Batch Normalization**
```

**Äáº·c Ä‘iá»ƒm quan trá»ng**:
- **No predictor**: Student vÃ  Teacher dÃ¹ng cÃ¹ng kiáº¿n trÃºc (khÃ¡c BYOL)
- **BN-free ViT**: KhÃ´ng dÃ¹ng Batch Normalization â†’ á»•n Ä‘á»‹nh hÆ¡n
- **Weight normalization**: Ãp dá»¥ng trÃªn output layer cá»§a projection head
- **K = 65,536**: Sá»‘ prototypes lá»›n, cho phÃ©p há»c representations phong phÃº

### CÃ¡c thÃ nh pháº§n chÃ­nh

| ThÃ nh pháº§n | MÃ´ táº£ | Tham sá»‘ chi tiáº¿t |
|------------|-------|------------------|
| **ViT backbone** | Vision Transformer | ViT-B: 86M params, 12 blocks, 12 heads |
| **Projection head** | MLP 3 layers | 768 â†’ 2048 â†’ 2048 â†’ K (weight normalized) |
| **K (prototypes)** | Sá»‘ "categories" áº©n | **65,536** dimensions |
| **Ï„ (temperature)** | Äá»™ sáº¯c nÃ©t softmax | Teacher: **0.04**, Student: **0.1** |
| **Î» (EMA)** | Tá»· lá»‡ giá»¯ Teacher | **0.996 â†’ 1.0** (cosine schedule) |
| **Centering momentum** | Tá»‘c Ä‘á»™ cáº­p nháº­t center | **m = 0.9** |

## 2.3 Loss Function - CÃ´ng thá»©c chi tiáº¿t

### Softmax vá»›i Temperature

**Student probability**:
```
P_s(x)[k] = exp(g_Î¸s(x)[k] / Ï„_s) / Î£_k' exp(g_Î¸s(x)[k'] / Ï„_s)
```

**Teacher probability** (vá»›i centering):
```
P_t(x)[k] = exp((g_Î¸t(x)[k] - c[k]) / Ï„_t) / Î£_k' exp((g_Î¸t(x)[k'] - c[k']) / Ï„_t)
```

**Cross-Entropy Loss**:
```
L = -Î£_k P_t(x)[k] Â· log P_s(x')[k]
```

Trong Ä‘Ã³:
- `x` = global crop (Teacher nhÃ¬n)
- `x'` = local hoáº·c global crop (Student nhÃ¬n)
- `g_Î¸` = projection head output (K dimensions)
- `c` = running mean center
- `Ï„_t = 0.04`, `Ï„_s = 0.1`

### Trá»±c quan

```
Teacher output (Ï„=0.04):    Student output (Ï„=0.1):
[0.95, 0.02, 0.01, ...]    [0.60, 0.20, 0.10, ...]
       â†“                           â†“
    Ráº¥t sáº¯c                    Má»m hÆ¡n
    (confident)               (uncertain)
```

Student Ä‘Æ°á»£c phÃ©p "má»m" hÆ¡n, nhÆ°ng pháº£i há»c theo hÆ°á»›ng cá»§a Teacher.

## 2.4 EMA: Exponential Moving Average

### CÃ´ng thá»©c

```
Î¸_T â† Î» Â· Î¸_T + (1-Î») Â· Î¸_S
```

**Cosine Schedule**:
```
Î»(t) = 1 - (1 - Î»_base) Ã— (1 + cos(Ï€t/T)) / 2

Î»_base = 0.996
t = current step
T = total steps
```

Vá»›i Î»_base = 0.996:
- **Báº¯t Ä‘áº§u**: Î» â‰ˆ 0.996 (Teacher cáº­p nháº­t 0.4% tá»« Student má»—i step)
- **Káº¿t thÃºc**: Î» â†’ 1.0 (Teacher gáº§n nhÆ° Ä‘Ã³ng bÄƒng)

### Táº¡i sao cáº§n EMA?

**Váº¥n Ä‘á»**: Náº¿u cáº£ Teacher vÃ  Student cÃ¹ng train báº±ng gradient:
- Cáº£ hai Ä‘uá»•i theo nhau
- Cuá»‘i cÃ¹ng má»i áº£nh â†’ cÃ¹ng 1 output (collapse)

**Giáº£i phÃ¡p**: Teacher thay Ä‘á»•i cá»±c cháº­m
- Teacher á»•n Ä‘á»‹nh â†’ Student cÃ³ má»¥c tiÃªu rÃµ rÃ ng
- NhÆ° tháº§y giÃ¡o kinh nghiá»‡m: khÃ´ng Ä‘á»•i Ã½ kiáº¿n theo tá»«ng cÃ¢u há»i

### Key Insight tá»« Paper: Teacher > Student

**Quan sÃ¡t quan trá»ng**: Teacher performance **tá»‘t hÆ¡n** Student trong suá»‘t quÃ¡ trÃ¬nh training!

```
Training Progress:
Step        Student Acc    Teacher Acc
10K         45%            52%
50K         62%            68%
100K        70%            75%
Final       75%            80.1%  â† DÃ¹ng Teacher weights
```

**Táº¡i sao?**
- Polyak-Ruppert averaging: EMA = ensemble cá»§a nhiá»u models
- Teacher = "trung bÃ¬nh" cá»§a táº¥t cáº£ Student versions
- Trung bÃ¬nh thÆ°á»ng tá»‘t hÆ¡n tá»«ng cÃ¡ thá»ƒ

## 2.4 Multi-crop Strategy

```
Tá»« 1 áº£nh, táº¡o:
- 2 global crops: 224Ã—224, cover >50% áº£nh
- 6 local crops: 96Ã—96, cover <50% áº£nh

Teacher: chá»‰ nháº­n global crops
Student: nháº­n cáº£ global vÃ  local crops
```

**Chi phÃ­ tÃ­nh toÃ¡n**:
- Local crop 96Â² = 9,216 pixels
- Global crop 224Â² = 50,176 pixels
- 6 local â‰ˆ 1 global vá» compute

**Káº¿t quáº£**: 8 gÃ³c nhÃ¬n khÃ¡c nhau, chá»‰ +50% compute so vá»›i 2 crops.

## 2.6 Centering vÃ  Sharpening - Chá»‘ng Collapse

### Collapse lÃ  gÃ¬?

Collapse = má»i áº£nh cho ra cÃ¹ng 1 output. CÃ³ 2 loáº¡i:

```
Mode Collapse:                    Uniform Collapse:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       â—         â”‚              â”‚ â— â— â— â— â— â— â— â— â”‚
â”‚      â—â—â—        â”‚              â”‚ â— â— â— â— â— â— â— â— â”‚
â”‚      â—â—â—        â”‚              â”‚ â— â— â— â— â— â— â— â— â”‚
â”‚       â—         â”‚              â”‚ â— â— â— â— â— â— â— â— â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Táº¥t cáº£ â†’ 1 Ä‘iá»ƒm                  Tráº£i Ä‘á»u â†’ khÃ´ng phÃ¢n biá»‡t
```

### Centering - CÃ´ng thá»©c chi tiáº¿t

**Update rule** (má»—i batch):
```
c â† mÂ·c + (1-m) Â· (1/B) Î£áµ¢ g_Î¸t(xáµ¢)

m = 0.9 (momentum)
B = batch size
g_Î¸t(xáµ¢) = teacher output cho sample i
```

**Ãp dá»¥ng vÃ o teacher**:
```
g_t(x) â† g_t(x) - c
```

**Táº¡i sao hoáº¡t Ä‘á»™ng?**
- Trá»« mean â†’ zero-centered
- KhÃ´ng cho 1 dimension dominate (táº¥t cáº£ outputs â‰ˆ cÃ¹ng 1 giÃ¡ trá»‹)
- Chá»‰ dÃ¹ng first-order batch statistics â†’ hoáº¡t Ä‘á»™ng vá»›i má»i batch size

**Side effect**: Centering khuyáº¿n khÃ­ch uniform collapse! (mean = 0 cho má»i áº£nh)

### Sharpening - CÃ´ng thá»©c chi tiáº¿t

```
Ï„_teacher = 0.04  (ráº¥t tháº¥p â†’ phÃ¢n bá»‘ sáº¯c)
Ï„_student = 0.1   (cao hÆ¡n â†’ phÃ¢n bá»‘ má»m)
```

**Softmax vá»›i temperature**:
```
softmax(x/Ï„) = exp(x/Ï„) / Î£ exp(x/Ï„)

Ï„ nhá» â†’ exp(x/Ï„) lá»›n â†’ winner-take-all
Ï„ lá»›n â†’ exp(x/Ï„) Ä‘á»u â†’ phÃ¢n bá»‘ Ä‘á»u
```

**VÃ­ dá»¥**:
```
Raw logits: [2.0, 1.5, 1.0, 0.5]

Ï„ = 1.0:  [0.47, 0.28, 0.17, 0.08]  â† má»m
Ï„ = 0.1:  [0.97, 0.02, 0.01, 0.00]  â† sáº¯c (gáº§n one-hot)
Ï„ = 0.04: [0.99, 0.01, 0.00, 0.00]  â† ráº¥t sáº¯c
```

### Centering + Sharpening = Balance

| Chá»‰ cÃ³ | Háº­u quáº£ |
|--------|---------|
| Centering alone | â†’ Uniform collapse (mean=0 nhÆ°ng Ä‘á»u) |
| Sharpening alone | â†’ Mode collapse (1 dimension dominate) |
| **Cáº£ hai** | **á»”n Ä‘á»‹nh**: peaked nhÆ°ng diverse |

### Ablation tá»« Paper

| Setting | ImageNet | Status |
|---------|----------|--------|
| Full DINO | **80.1%** | âœ“ á»”n Ä‘á»‹nh |
| Bá» EMA (train cáº£ Teacher) | â€” | âœ— Collapse ngay |
| Bá» Centering | â€” | âœ— Collapse epoch 1 |
| Bá» Sharpening (Ï„_t = Ï„_s = 0.1) | â€” | âœ— Collapse cháº­m |
| Bá» Multi-crop | 72.3% | âœ“ á»”n Ä‘á»‹nh nhÆ°ng kÃ©m |

## 2.6 Emerging Properties

### Attention Map Tá»± Segment

PhÃ¡t hiá»‡n báº¥t ngá»: attention heads tá»± há»c cÃ¡ch tÃ¡ch váº­t thá»ƒ khá»i ná»n.

```
Head 1: Focus vÃ o Ä‘áº§u con chÃ³
Head 2: Focus vÃ o thÃ¢n con chÃ³
Head 3: Focus vÃ o ná»n (cá», trá»i)
```

**Táº¡i sao xáº£y ra?**
- Supervised: Chá»‰ cáº§n biáº¿t "Ä‘Ã¢y lÃ  chÃ³" â†’ khÃ´ng cáº§n hiá»ƒu Ä‘Ã¢u lÃ  Ä‘áº§u
- DINO: So sÃ¡nh local/global crop â†’ buá»™c pháº£i hiá»ƒu vÃ¹ng nÃ o quan trá»ng

### Káº¿t quáº£ v1

| Model | ImageNet Linear Probe |
|-------|----------------------|
| ViT-S/8 | 79.7% |
| ViT-B/8 | **80.1%** |
| ResNet-50 (DINO) | 75.3% |
| ResNet-50 (Supervised) | 76.5% |

**Milestone**: Láº§n Ä‘áº§u self-supervised vÆ°á»£t supervised!

---

# Part 3: DINOv2 (2023)

## 3.1 Motivation: 4 Háº¡n Cháº¿ cá»§a v1

| Háº¡n cháº¿ v1 | Giáº£i phÃ¡p v2 |
|------------|--------------|
| Data nhá» (1.28M) | LVD-142M (Ã—110) |
| Model nhá» (86M) | ViT-g (1.1B, Ã—13) |
| Chá»‰ classification | Dense tasks (seg, depth) |
| Chá»‰ DINO loss | DINO + iBOT + KoLeo |

## 3.2 LVD-142M: Data Curation Pipeline Chi Tiáº¿t

### Pipeline vá»›i sá»‘ liá»‡u cá»¥ thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA CURATION PIPELINE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1.2B raw images (web crawl)                                    â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼  Safety filtering                                      â”‚
â”‚         â”‚  â€¢ NSFW classifier                                     â”‚
â”‚         â”‚  â€¢ Restricted domains blacklist                        â”‚
â”‚         â”‚                                                        â”‚
â”‚  1.1B images                                                     â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼  PCA hash deduplication                                â”‚
â”‚         â”‚  â€¢ Exact & near-exact duplicates                       â”‚
â”‚         â”‚                                                        â”‚
â”‚  744M images                                                     â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼  Copy-detection deduplication                          â”‚
â”‚         â”‚  â€¢ SSCD model (Self-Supervised Copy Detection)         â”‚
â”‚         â”‚  â€¢ Cosine similarity > 0.6                             â”‚
â”‚         â”‚  â€¢ k=64 nearest neighbors checked                      â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼  Benchmark leak removal                                â”‚
â”‚         â”‚  â€¢ Remove images similar to test sets                  â”‚
â”‚         â”‚  â€¢ Cosine similarity > 0.45                            â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼  Self-supervised retrieval                             â”‚
â”‚         â”‚  â€¢ ViT-H/16 pretrained features                        â”‚
â”‚         â”‚  â€¢ k-means: 100,000 clusters                           â”‚
â”‚         â”‚  â€¢ Sample from each cluster                            â”‚
â”‚         â”‚                                                        â”‚
â”‚  142M curated images (LVD-142M)                                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Implementation: Faiss library, GPU-accelerated
Hardware: 20 nodes Ã— 8 V100 GPUs
```

### Key insight: Quality > Quantity

| Dataset | Size | ImageNet Linear | ADE20k mIoU |
|---------|------|-----------------|-------------|
| Raw uncurated | 1.2B | 84.2% | 46.3 |
| **LVD-142M** | **142M** | **86.5%** | **49.0** |
| Ratio | Ã—0.12 | +2.3% | +2.7 |

**Ãt hÆ¡n 8Ã— nhÆ°ng tá»‘t hÆ¡n trÃªn Táº¤T Cáº¢ benchmarks!**

**Táº¡i sao?**
```
Raw 1.2B data problems:          Curated 142M:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—â—â—â—â—â—â—â—â—â—â—â— (dupes)â”‚          â”‚ â— â— â— â— â— â— â—      â”‚
â”‚ NSFW content       â”‚    â†’      â”‚ Clean, diverse     â”‚
â”‚ Logos, watermarks  â”‚          â”‚ Balanced classes   â”‚
â”‚ Biased distributionâ”‚          â”‚ No benchmark leaks â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.3 Three Losses - Chi tiáº¿t cÃ´ng thá»©c

### DINO Loss (káº¿ thá»«a tá»« v1)

```
L_DINO = -Î£_k P_t(x)[k] Â· log P_s(x')[k]
```

Ãp dá»¥ng trÃªn **CLS token** â†’ hiá»ƒu global semantics.

### iBOT Loss - Masked Patch Prediction

**Ã tÆ°á»Ÿng cá»‘t lÃµi**: Che patches, dá»± Ä‘oÃ¡n **semantic token** (khÃ´ng pháº£i pixel).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         iBOT Architecture                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Student input:    [CLS] [P1] [MASK] [P3] [MASK] [P5] ...   â”‚
â”‚                                 â†“           â†“                â”‚
â”‚  Student predicts:         p_s(i)       p_s(j)              â”‚
â”‚                                 â†“           â†“                â”‚
â”‚                           Cross-entropy loss                 â”‚
â”‚                                 â†‘           â†‘                â”‚
â”‚  Teacher targets:          p_t(i)       p_t(j)              â”‚
â”‚                                 â†‘           â†‘                â”‚
â”‚  Teacher input:    [CLS] [P1] [P2]  [P3] [P4]  [P5] ...     â”‚
â”‚                          (FULL image - no masking)           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CÃ´ng thá»©c**:
```
L_iBOT = -Î£áµ¢âˆˆM p_t(i) Â· log(p_s(i))

M = set of masked patch indices
p_t = Sinkhorn-Knopp centered teacher probability
p_s = Student softmax probability
```

**So sÃ¡nh MAE vs iBOT**:

| Aspect | MAE | iBOT |
|--------|-----|------|
| **Target** | Pixel values (RGB) | Prototype scores from teacher |
| **Loss** | MSE reconstruction | Cross-entropy |
| **Level** | Low-level (texture) | High-level (semantic) |
| **Features** | Requires finetuning | **Works frozen** |
| **VÃ­ dá»¥** | "Pixel mÃ u xanh" | "ÄÃ¢y lÃ  pháº§n tai chÃ³" |

**Táº¡i sao quan trá»ng?**
- DINO loss: Chá»‰ hiá»ƒu global (CLS token)
- iBOT loss: Hiá»ƒu local (tá»«ng patch)
- **Cáº§n cáº£ hai** cho dense tasks (segmentation)

**At scale**: UNTIED heads work better (separate DINO and iBOT projection heads)

### KoLeo Loss - Uniform Distribution

**Váº¥n Ä‘á»**: Embeddings cÃ³ thá»ƒ tá»¥ láº¡i thÃ nh clusters â†’ máº¥t Ä‘a dáº¡ng.

**CÃ´ng thá»©c** (Kozachenko-Leonenko differential entropy estimator):
```
L_KoLeo = -(1/n) Î£áµ¢ log(d_{n,i})

d_{n,i} = min_{jâ‰ i} ||xáµ¢ - xâ±¼||   (L2 distance to nearest neighbor)
xáµ¢ = L2-normalized CLS features
```

**Trá»±c quan**:
```
Before KoLeo:          After KoLeo:
    â—â—â—                    â—      â—
   â—â—â—â—â—      â†’              â—  â—
    â—â—â—                  â—      â—
(clustering)          (uniformly spread)
```

**Implementation details**:
- Weight: **Î»_KoLeo = 0.1**
- Applied on: CLS tokens only
- Global crop: First global crop only
- Gradient: Maximizes distance to nearest neighbor

### Combined Loss

```
L_total = L_DINO + L_iBOT + 0.1 Ã— L_KoLeo
```

| Loss | Target | Purpose | Operates on |
|------|--------|---------|-------------|
| DINO | CLS token | Global semantics | CLS only |
| iBOT | Masked patches | Dense/local features | Patch tokens |
| KoLeo | Batch diversity | Prevent collapse | CLS only |

### Ablation Results

| Bá» loss nÃ o? | ImageNet Linear | ADE20k mIoU | Î” mIoU |
|--------------|-----------------|-------------|--------|
| **Full (baseline)** | **86.5%** | **49.0** | â€” |
| Bá» iBOT | 86.3% | 44.8 | **âˆ’4.2** |
| Bá» KoLeo | 86.0% | 48.5 | âˆ’0.5 |
| Bá» cáº£ iBOT+KoLeo | 85.8% | 42.1 | âˆ’6.9 |

**Key insight**: iBOT quan trá»ng nháº¥t cho dense tasks!

## 3.4 Register Tokens

### Váº¥n Ä‘á»: Attention Artifacts

Khi scale ViT lÃªn lá»›n + high resolution:
- Má»™t sá»‘ positions nháº­n attention khÃ´ng há»£p lÃ½
- Attention map cÃ³ "vÃ¹ng cháº¿t"

### Giáº£i phÃ¡p

ThÃªm 4-8 learnable tokens (khÃ´ng gáº¯n vá»›i patch nÃ o):

```
[CLS] [REG1] [REG2] [REG3] [REG4] [P1] [P2] ... [P196]
       â†‘      â†‘      â†‘      â†‘
    "Parking spots" cho extra attention
```

Register tokens Ä‘Ã³ng vai trÃ² "bÃ£i Ä‘á»—" â€” hÃºt attention thá»«a, lÃ m sáº¡ch attention map cho cÃ¡c patches tháº­t.

## 3.5 Model Scaling

### ViT Variants

| Model | Params | Blocks | Embed Dim | FFN | Heads |
|-------|--------|--------|-----------|-----|-------|
| ViT-S/14 | 22M | 12 | 384 | MLP | 6 |
| ViT-B/14 | 86M | 12 | 768 | MLP | 12 |
| ViT-L/14 | 307M | 24 | 1024 | MLP | 16 |
| **ViT-g/14** | **1.1B** | **40** | **1536** | **SwiGLU** | **24** |

### Patch Size: 14 vs 16

| Patch | Resolution | Patches | Compute | Detail |
|-------|------------|---------|---------|--------|
| /16 | 224Ã—224 | 196 | Lower | Less |
| **/14** | 224Ã—224 | **256** | **Higher** | **More** |

DINOv2 chá»n **/14** Ä‘á»ƒ cÃ³ Ä‘á»™ phÃ¢n giáº£i cao hÆ¡n cho dense tasks.

## 3.6 Káº¿t Quáº£ v2

### So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| Benchmark | DINOv2 (ViT-g) | iBOT | MAE | OpenCLIP |
|-----------|----------------|------|-----|----------|
| ImageNet Linear | **86.5%** | 82.3% | 73.5% | 83.5% |
| ADE20k mIoU (frozen) | **49.0** | 44.8 | â€” | â€” |
| ADE20k mIoU (Mask2Former) | **60.2** | â€” | â€” | â€” |
| Oxford Retrieval mAP | **+41%** | baseline | â€” | â€” |

### Scaling Law

| Model | Params | ImageNet | ADE20k |
|-------|--------|----------|--------|
| ViT-S | 22M | 81.1% | 42.5 |
| ViT-B | 86M | 84.5% | 45.8 |
| ViT-L | 307M | 86.3% | 48.2 |
| **ViT-g** | **1.1B** | **86.5%** | **49.0** |

**DINOv2 = Foundation model cho vision**
- 1 backbone dÃ¹ng cho nhiá»u tasks
- **KhÃ´ng cáº§n fine-tune** (frozen features)
- VÆ°á»£t táº¥t cáº£ phÆ°Æ¡ng phÃ¡p SSL khÃ¡c

---

# Part 4: DINOv3 (2025)

## 4.1 Scaling Challenge - KhÃ´ng Pháº£i Divergence!

### Scale

| Aspect | v2 | v3 | Scale |
|--------|-----|-----|-------|
| Model | 1.1B | **6.7B** | Ã—6 |
| Data | 142M | **1.69B** | Ã—12 |

### Váº¥n Ä‘á» thá»±c sá»±: Dense Feature DEGRADATION

**KHÃ”NG pháº£i** divergence hay loss explosion thÃ´ng thÆ°á»ng!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  THE REAL PROBLEM                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Segmentation Performance (ADE20k mIoU):                    â”‚
â”‚                                                              â”‚
â”‚  mIoU â†‘                                                      â”‚
â”‚   50 â”‚        â—â—â—â—                                          â”‚
â”‚   45 â”‚      â—      â—â—                                       â”‚
â”‚   40 â”‚    â—            â—â—                                   â”‚
â”‚   35 â”‚  â—                  â—â—â—â—â—â—                           â”‚
â”‚   30 â”‚â—                              â—â—â—â—â—                  â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training steps â”‚
â”‚         0     100k   200k   300k   400k   500k              â”‚
â”‚                  â†‘                                           â”‚
â”‚           PEAK at ~200k, then DECLINES!                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**NguyÃªn nhÃ¢n**:
1. CLS token vÃ  patch outputs trá»Ÿ nÃªn **quÃ¡ giá»‘ng nhau**
2. Patches "sá»¥p Ä‘á»•" vá» phÃ­a global summary (CLS)
3. Máº¥t **local specificity** â†’ dense tasks suffer
4. DINO global loss **dominate** over iBOT patch-level loss

**Key insight**: Problem khÃ´ng pháº£i training khÃ´ng há»™i tá»¥, mÃ  lÃ  **há»™i tá»¥ sai**!

## 4.2 Gram Anchoring - Derivation Chi Tiáº¿t

### Gram Matrix lÃ  gÃ¬?

**Äá»‹nh nghÄ©a**:
```
Cho X = P Ã— d matrix (P patches, d dimensions)
Má»—i hÃ ng = 1 patch feature (L2-normalized)

G = X Â· Xáµ€   (P Ã— P matrix)

G[i,j] = cos_sim(patch_i, patch_j)
       = <xáµ¢, xâ±¼>  (vÃ¬ Ä‘Ã£ L2-normalized)
```

**Trá»±c quan**:
```
Feature matrix X:              Gram matrix G:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ patch1 features â”‚           â”‚ 1.0  0.8  0.3 ..â”‚  â† patch1 sim vá»›i táº¥t cáº£
â”‚ patch2 features â”‚    â†’      â”‚ 0.8  1.0  0.5 ..â”‚  â† patch2 sim vá»›i táº¥t cáº£
â”‚ patch3 features â”‚           â”‚ 0.3  0.5  1.0 ..â”‚
â”‚ ...             â”‚           â”‚ ...             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    P Ã— d                          P Ã— P
                              (pairwise similarities)
```

### Gram Anchoring Loss

```
X_S = Student patch features (P Ã— d, L2-normalized)
X_G = Gram teacher patch features (P Ã— d, L2-normalized)

L_Gram = ||X_S Â· X_Sáµ€ - X_G Â· X_Gáµ€||Â²_F

       = ||G_student - G_anchor||Â²_F
```

**Gram Teacher**: Checkpoint tá»« **200k iterations** (khi dense features tá»‘t nháº¥t)

### Táº¡i sao Gram Anchoring hoáº¡t Ä‘á»™ng?

**Key Insight**: Constrain **similarity structure**, NOT specific values!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  What Gram Loss ALLOWS:                                    â”‚
â”‚  â€¢ Feature rotation                                        â”‚
â”‚  â€¢ Feature scaling                                         â”‚
â”‚  â€¢ Feature translation                                     â”‚
â”‚                                                            â”‚
â”‚  What Gram Loss PRESERVES:                                 â”‚
â”‚  â€¢ Pairwise similarities                                   â”‚
â”‚  â€¢ Relative geometry                                       â”‚
â”‚  â€¢ "patch_eye similar to patch_ear more than patch_sky"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### High-Resolution Gram (L_HRef)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HIGH-RES GRAM                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input: 224Ã—224                Input: 448Ã—448 (2Ã—)          â”‚
â”‚       â†“                             â†“                        â”‚
â”‚  Student ViT                   Gram Teacher ViT              â”‚
â”‚       â†“                             â†“                        â”‚
â”‚  14Ã—14 feature map            28Ã—28 feature map             â”‚
â”‚       â†“                             â†“                        â”‚
â”‚       â”‚                      Bicubic downsample             â”‚
â”‚       â”‚                             â†“                        â”‚
â”‚       â”‚                       14Ã—14 feature map             â”‚
â”‚       â”‚                             â”‚                        â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€ L_Gram Loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Gain: +2 mIoU on ADE20k (high-res details!)
```

### Refinement Loss

```
L_Ref = w_D Ã— L_DINO + L_iBOT + w_DK Ã— L_DKoleo + w_Gram Ã— L_Gram

w_Gram = 2.0
```

### Ablation

| Setting | ADE20k mIoU | Status |
|---------|-------------|--------|
| Without Gram Anchoring | Peaks at 200k, then **degrades** | âœ— |
| With Gram Anchoring | **Stable improvement** to 1M+ steps | âœ“ |
| + High-Res Gram | **+2 mIoU** additional | âœ“âœ“ |

**Effect is almost immediate**: Improvements within 10k iterations!

## 4.3 Text Alignment vs CLIP

### So sÃ¡nh chi tiáº¿t

| Aspect | CLIP | DINOv3 |
|--------|------|--------|
| **Vision encoder** | Trained jointly with text | **FROZEN** (pretrained DINO) |
| **What's trained** | Both encoders | Text encoder + 2 adapter layers |
| **Features used** | CLS only | **CLS + mean-pooled patches** |
| **Dense capability** | Poor | **Excellent** |
| **Data needed** | 400M image-text pairs | Images only, text optional |
| **Vision bias** | Biased toward text | Pure visual |

### DINOv3 Two-Phase Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DECOUPLED TRAINING                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Phase 1: Pure Visual Learning                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ DINO + iBOT â”‚  â† No text, no language bias               â”‚
â”‚  â”‚ + KoLeo     â”‚  â† Pure visual understanding                â”‚
â”‚  â”‚ + Gram      â”‚                                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â†“                                                    â”‚
â”‚         â†“  FREEZE vision encoder                            â”‚
â”‚         â†“                                                    â”‚
â”‚  Phase 2: Text Alignment                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Frozen DINO ViT â†’ CLS + patch_mean â†’ Adapter â†’     â”‚    â”‚
â”‚  â”‚                                           â†“          â”‚    â”‚
â”‚  â”‚  Text Encoder â†’ Text embed â†’ Contrastive Loss       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  "Learn to see first, learn to talk later"                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Æ¯u Ä‘iá»ƒm**:
- Vision features = **pure visual understanding**
- ThÃªm text **khÃ´ng lÃ m há»ng** vision performance
- Dense features **preserved** (CLIP loses them)

## 4.4 ViT-7B Architecture

### So sÃ¡nh vá»›i DINOv2

| | DINOv2 (ViT-g) | DINOv3 (ViT-7B) |
|---|----------------|-----------------|
| **Parameters** | 1.1B | **6.7B** |
| **Patch size** | 14 | **16** |
| **Position embed** | Learnable | **Axial RoPE** |
| **Embed dimension** | 1536 | **4096** |
| **Blocks** | 40 | **48** |
| **FFN** | SwiGLU | SwiGLU |
| **Prototypes (DINO)** | 128k | **256k** |
| **Prototypes (iBOT)** | 128k | **96k** |

### Axial RoPE (Rotary Position Embedding)

```
Standard positional embedding:    Axial RoPE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Learned 2D grid         â”‚      â”‚ Factorized: row Ã— col   â”‚
â”‚ Fixed resolution        â”‚  â†’   â”‚ Extrapolates to any res â”‚
â”‚ Doesn't generalize      â”‚      â”‚ Rotation-based          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Æ¯u Ä‘iá»ƒm**: Generalizes to higher resolutions without retraining!

## 4.5 Training Details

### Hardware & Compute

| Resource | Value |
|----------|-------|
| GPUs | **256 Ã— H100** |
| GPU Hours | **61,440** |
| CO2 Emission | ~18 tCO2eq |
| Training time | ~10 days |

### Hyperparameters

| Parameter | Value |
|-----------|-------|
| Learning rate | **0.0004** (constant, no cosine) |
| Batch size | **4096** images |
| Crops per image | 2 global + 8 local |
| Total crops | 40,960 crops/step |

### Dataset: LVD-1689M

```
Source: 17B Instagram images
    â†“  Safety + dedup + curation
LVD-1689M (Ã—12 larger than v2)
```

| Dataset | Size | Source |
|---------|------|--------|
| LVD-142M (v2) | 142M | Web crawl |
| **LVD-1689M (v3)** | **1.69B** | **Instagram** |

## 4.6 Káº¿t Quáº£ DINOv3

### So sÃ¡nh vá»›i DINOv2

| Benchmark | DINOv2 (ViT-g) | DINOv3 (ViT-7B) | Î” |
|-----------|----------------|-----------------|-----|
| ImageNet Linear | 86.5% | **88.4%** | +1.9 |
| ADE20k mIoU (linear) | 49.0 | **55.9** | **+6.9** |
| ADE20k mIoU (full) | 60.2 | **63.0** | +2.8 |
| COCO Detection mAP | 62.5 | **66.1** | +3.6 |
| DAVIS Tracking J&F | 76.6 | **83.3** | **+6.7** |
| ObjectNet | 66.0 | **79.0** | **+13.0** |

### Key Insight: Scaling Benefits Dense Tasks Most

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHERE SCALING HELPS MOST                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Task           v2 â†’ v3    Improvement                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  Classification  86.5 â†’ 88.4   +1.9%  â† Near saturation   â”‚
â”‚  Segmentation    49.0 â†’ 55.9   +6.9   â† BIG GAIN          â”‚
â”‚  Video Tracking  76.6 â†’ 83.3   +6.7   â† BIG GAIN          â”‚
â”‚  ObjectNet       66.0 â†’ 79.0   +13.0  â† HUGE GAIN         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Táº¡i sao?**
- **Classification**: Gáº§n bÃ£o hÃ²a (88% â†’ khÃ³ tÄƒng thÃªm)
- **Segmentation**: Cáº§n hiá»ƒu tá»«ng pixel â†’ scale giÃºp nhiá»u
- **Video/Tracking**: Temporal understanding â†’ scale giÃºp nhiá»u
- **ObjectNet**: Out-of-distribution â†’ cáº§n representations tá»‘t hÆ¡n

### DINOv3 = First SSL at Weakly-Supervised Parity

```
ImageNet Accuracy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                 â”‚
â”‚  Supervised (labels):           85.7%          â”‚
â”‚  Weakly-supervised (hashtags):  88.5%          â”‚
â”‚  DINOv3 (NO labels):           88.4%  â† WOW!  â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### HÆ°á»›ng Ä‘i tÆ°Æ¡ng lai

**KhÃ´ng cáº§n scale classification ná»¯a**. Focus vÃ o:
- Dense prediction (segmentation, depth)
- Video understanding
- 3D vision
- Multimodal (with preserved dense features)

---

# Appendices

## A. Vietnamese Glossary

| English | Vietnamese | Äá»‹nh nghÄ©a |
|---------|------------|------------|
| Attention | CÆ¡ cháº¿ chÃº Ã½ | CÃ¡ch model quyáº¿t Ä‘á»‹nh nÃªn "nhÃ¬n" vÃ o Ä‘Ã¢u |
| Attention head | Äáº§u attention | Má»™t gÃ³c nhÃ¬n Ä‘á»™c láº­p trong multi-head attention |
| CLS token | Token phÃ¢n loáº¡i | Vector Ä‘áº¡i diá»‡n cho toÃ n bá»™ áº£nh |
| Collapse | Sá»¥p Ä‘á»• | Lá»—i khi má»i áº£nh cho ra cÃ¹ng output |
| Contrastive | TÆ°Æ¡ng pháº£n | PhÆ°Æ¡ng phÃ¡p kÃ©o giá»‘ng gáº§n, Ä‘áº©y khÃ¡c xa |
| Cross-entropy | Entropy chÃ©o | Loss Ä‘o sá»± khÃ¡c biá»‡t giá»¯a 2 phÃ¢n bá»‘ |
| Dense prediction | Dá»± Ä‘oÃ¡n dÃ y | Dá»± Ä‘oÃ¡n cho tá»«ng pixel (segmentation, depth) |
| Distillation | ChÆ°ng cáº¥t | Chuyá»ƒn tri thá»©c tá»« model lá»›n sang nhá» |
| Embedding | NhÃºng / Vector Ä‘áº·c trÆ°ng | Biá»ƒu diá»…n dá»¯ liá»‡u dÆ°á»›i dáº¡ng vector sá»‘ |
| EMA | Trung bÃ¬nh Ä‘á»™ng | CÃ¡ch cáº­p nháº­t Teacher tá»« Student |
| Foundation model | Model ná»n táº£ng | Model pretrained dÃ¹ng cho nhiá»u tasks |
| Gram matrix | Ma tráº­n Gram | FFáµ€, Ä‘o tÆ°Æ¡ng quan giá»¯a features |
| iBOT | iBOT | Masked prediction á»Ÿ má»©c semantic |
| KoLeo | KoLeo | Regularization Ä‘áº©y embeddings tráº£i Ä‘á»u |
| Linear probe | Äáº§u tuyáº¿n tÃ­nh | Chá»‰ train 1 layer linear trÃªn frozen features |
| mIoU | mIoU | Mean Intersection over Union (Ä‘o segmentation) |
| Multi-crop | Cáº¯t nhiá»u gÃ³c | Táº¡o nhiá»u views tá»« 1 áº£nh |
| Patch | Máº£nh | Má»™t pháº§n nhá» cá»§a áº£nh (16Ã—16 pixels) |
| Positional encoding | MÃ£ hÃ³a vá»‹ trÃ­ | Cho model biáº¿t patch á»Ÿ Ä‘Ã¢u |
| Projection head | Äáº§u chiáº¿u | MLP sau ViT, chuyá»ƒn Ä‘á»•i features |
| Register token | Token Ä‘Äƒng kÃ½ | Learnable tokens Ä‘á»ƒ "hÃºt" extra attention |
| Self-supervised | Tá»± giÃ¡m sÃ¡t | Há»c khÃ´ng cáº§n nhÃ£n ngÆ°á»i gÃ¡n |
| Sharpening | LÃ m sáº¯c | DÃ¹ng temperature tháº¥p cho phÃ¢n bá»‘ sáº¯c nÃ©t |
| Softmax | Softmax | Chuyá»ƒn sá»‘ thÃ´ thÃ nh xÃ¡c suáº¥t (tá»•ng = 1) |
| Temperature | Nhiá»‡t Ä‘á»™ | Kiá»ƒm soÃ¡t Ä‘á»™ sáº¯c/má»m cá»§a phÃ¢n bá»‘ |
| Transformer | Transformer | Kiáº¿n trÃºc dÃ¹ng attention |
| ViT | Vision Transformer | Transformer cho áº£nh |

## B. Math Notation Reference

| Notation | Meaning |
|----------|---------|
| Î¸_T, Î¸_S | Parameters cá»§a Teacher, Student |
| P_t, P_s | Output probability cá»§a Teacher, Student |
| Ï„ | Temperature (Ï„_t=0.04, Ï„_s=0.1) |
| Î» | EMA coefficient (0.996 â†’ 1.0) |
| K | Sá»‘ prototypes (65,536 trong v1, 256k trong v3) |
| d | Dimension (768 trong ViT-B, 4096 trong ViT-7B) |
| Q, K, V | Query, Key, Value trong attention |
| G | Gram matrix (G = XÂ·Xáµ€) |
| L | Loss function |
| c | Centering vector (running mean) |
| m | Centering momentum (0.9) |
| M | Set of masked patch indices (iBOT) |
| d_{n,i} | Distance to nearest neighbor (KoLeo) |
| X_S, X_G | Student/Gram teacher features |
| P | Number of patches |
| B | Batch size |

## C. Further Reading

### Papers
1. Caron et al. ICCV 2021 â€” DINOv1
2. Oquab et al. TMLR 2024 â€” DINOv2
3. SimÃ©oni et al. arXiv 2025 â€” DINOv3
4. Dosovitskiy et al. ICLR 2021 â€” ViT
5. Hinton et al. arXiv 2015 â€” Knowledge Distillation
6. Zhou et al. ICLR 2022 â€” iBOT
7. Darcet et al. ICLR 2024 â€” Register Tokens

### Code
- DINOv2: https://github.com/facebookresearch/dinov2
- ViT: https://github.com/google-research/vision_transformer

---

*TÃ i liá»‡u nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ cho CS students cÃ³ ML cÆ¡ báº£n. Má»i concept Ä‘á»u cÃ³ vÃ­ dá»¥ trÆ°á»›c, lÃ½ thuyáº¿t sau.*
