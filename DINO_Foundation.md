# DINO: Self-Distillation with NO Labels
## TÃ i liá»‡u tham kháº£o toÃ n diá»‡n

> **Äá»‘i tÆ°á»£ng**: CS students cÃ³ kiáº¿n thá»©c ML cÆ¡ báº£n (neural network, loss function, gradient descent)
> **PhÆ°Æ¡ng phÃ¡p**: Examples first, theory second

---

# Part 0: DINO LÃ m ÄÆ°á»£c GÃ¬?

## 0.1 Demo: Attention Maps Tá»± Segment Váº­t Thá»ƒ

TrÆ°á»›c khi Ä‘i vÃ o lÃ½ thuyáº¿t, hÃ£y xem DINO lÃ m Ä‘Æ°á»£c gÃ¬.

**VÃ­ dá»¥ 1: Attention map trÃªn áº£nh con chÃ³**
```
[INPUT]                    [ATTENTION MAPS]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚           â”‚ Head 1  â”‚ â”‚ Head 2  â”‚ â”‚ Head 3  â”‚
â”‚   ğŸ• Dog    â”‚    â†’      â”‚  Äáº§u    â”‚ â”‚  ThÃ¢n   â”‚ â”‚  Ná»n    â”‚
â”‚             â”‚           â”‚ (sÃ¡ng)  â”‚ â”‚ (sÃ¡ng)  â”‚ â”‚ (sÃ¡ng)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

KhÃ´ng ai dáº¡y model "Ä‘Ã¢y lÃ  Ä‘áº§u", "Ä‘Ã¢y lÃ  thÃ¢n" â€” nÃ³ tá»± há»c phÃ¢n biá»‡t!

**VÃ­ dá»¥ 2: Semantic segmentation khÃ´ng cáº§n annotation**
- Input: áº¢nh xe hÆ¡i trÃªn Ä‘Æ°á»ng
- Output: Tá»± Ä‘á»™ng tÃ¡ch xe, Ä‘Æ°á»ng, báº§u trá»i thÃ nh cÃ¡c vÃ¹ng riÃªng biá»‡t
- KhÃ´ng cáº§n 1 pixel annotation nÃ o

## 0.2 So SÃ¡nh Chi PhÃ­

| PhÆ°Æ¡ng phÃ¡p | Labels cáº§n | Chi phÃ­ Æ°á»›c tÃ­nh | Thá»i gian |
|-------------|------------|------------------|-----------|
| **ImageNet Supervised** | 14M áº£nh Ã— 3 ngÆ°á»i kiá»ƒm tra | $500K+ | 2 nÄƒm |
| **OpenCLIP** | 400M text-image pairs | Cáº§n crawl + filter | ThÃ¡ng |
| **DINO** | 0 | Chá»‰ cáº§n áº£nh raw | Tuáº§n |

## 0.3 TÃ³m Táº¯t 1 PhÃºt

DINO = **D**istillation with **NO** labels

**Ã tÆ°á»Ÿng cá»‘t lÃµi**:
1. Táº¡o 2 phiÃªn báº£n cá»§a cÃ¹ng 1 model: Teacher vÃ  Student
2. Teacher nhÃ¬n toÃ n cáº£nh, Student nhÃ¬n cá»¥c bá»™
3. Student pháº£i Ä‘oÃ¡n output giá»‘ng Teacher
4. KhÃ´ng cáº§n nhÃ£n â€” Teacher chÃ­nh lÃ  "Ä‘Ã¡p Ã¡n"

**Káº¿t quáº£**: 88.4% ImageNet accuracy, vÆ°á»£t supervised learning (85.7%)

---

# Part 1: Prerequisites

## 1.1 Tá»« áº¢nh Äáº¿n Patches

### VÃ­ dá»¥ trÆ°á»›c

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n cÃ³ má»™t bá»©c áº£nh 224Ã—224 pixels. Thay vÃ¬ xá»­ lÃ½ tá»«ng pixel má»™t (224Ã—224 = 50,176 pixels â€” quÃ¡ nhiá»u!), ta "cáº¯t" áº£nh thÃ nh cÃ¡c máº£nh nhá» hÆ¡n.

```
áº¢nh gá»‘c 224Ã—224
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€...â”€â”¬â”€â”€â”€â”€â”
â”‚ P1 â”‚ P2 â”‚ P3 â”‚     â”‚P14 â”‚  â† HÃ ng 1: 14 patches
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€...â”€â”¼â”€â”€â”€â”€â”¤
â”‚P15 â”‚P16 â”‚P17 â”‚     â”‚P28 â”‚  â† HÃ ng 2: 14 patches
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€...â”€â”¼â”€â”€â”€â”€â”¤
â”‚... â”‚    â”‚    â”‚     â”‚... â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€...â”€â”¼â”€â”€â”€â”€â”¤
â”‚    â”‚    â”‚    â”‚     â”‚P196â”‚  â† HÃ ng 14: 14 patches
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€...â”€â”´â”€â”€â”€â”€â”˜

Má»—i patch: 16Ã—16 pixels
Tá»•ng: 14Ã—14 = 196 patches
```

### Táº¡i sao 16Ã—16?

- QuÃ¡ nhá» (4Ã—4): 3,136 patches â†’ sequence quÃ¡ dÃ i
- QuÃ¡ lá»›n (32Ã—32): 49 patches â†’ máº¥t chi tiáº¿t
- 16Ã—16: Balance giá»¯a chi tiáº¿t vÃ  Ä‘á»™ dÃ i sequence

### Tá»« Patch Äáº¿n Vector

Má»—i patch 16Ã—16Ã—3 (RGB) = 768 sá»‘ â†’ **Linear projection** â†’ Vector 768 chiá»u

```python
# Pseudocode
patch = image[0:16, 0:16, :]  # 16Ã—16Ã—3 = 768 numbers
embedding = linear_layer(patch.flatten())  # â†’ 768-dim vector
```

**Thuáº­t ngá»¯**:
- **Patch**: Má»™t máº£nh nhá» cá»§a áº£nh (16Ã—16 pixels)
- **Embedding**: Vector sá»‘ Ä‘áº¡i diá»‡n cho patch
- **Positional encoding**: Cho model biáº¿t patch á»Ÿ vá»‹ trÃ­ nÃ o (thÃªm vÃ o embedding)

## 1.2 Attention Mechanism

### VÃ­ dá»¥: TÃ¬m thÃ´ng tin trong thÆ° viá»‡n

Báº¡n muá»‘n tÃ¬m sÃ¡ch vá» "machine learning". QuÃ¡ trÃ¬nh:

1. **Query (Q)**: CÃ¢u há»i cá»§a báº¡n â€” "machine learning"
2. **Key (K)**: TiÃªu Ä‘á»/tag cá»§a má»—i cuá»‘n sÃ¡ch
3. **Value (V)**: Ná»™i dung thá»±c sá»± cá»§a má»—i cuá»‘n sÃ¡ch

```
Báº¡n há»i: "machine learning" (Query)
                    â†“
         So sÃ¡nh vá»›i má»—i Key
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SÃ¡ch 1: "Deep Learning" â†’ Match cao    â”‚ â†’ Láº¥y nhiá»u ná»™i dung
â”‚ SÃ¡ch 2: "Statistics"    â†’ Match vá»«a    â”‚ â†’ Láº¥y Ã­t ná»™i dung
â”‚ SÃ¡ch 3: "Cooking"       â†’ Match tháº¥p   â”‚ â†’ Gáº§n nhÆ° bá» qua
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         Tá»•ng há»£p ná»™i dung (Values)
                    â†“
              CÃ¢u tráº£ lá»i
```

### CÃ´ng thá»©c Attention

```
Attention(Q, K, V) = softmax(QK^T / âˆšd) Ã— V
```

Giáº£i thÃ­ch tá»«ng pháº§n:

| Pháº§n | Ã nghÄ©a | VÃ­ dá»¥ |
|------|---------|-------|
| `QK^T` | Äo Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a Q vÃ  K | "machine learning" khá»›p vá»›i "Deep Learning" bao nhiÃªu? |
| `âˆšd` | Chia Ä‘á»ƒ á»•n Ä‘á»‹nh (d=64 trong ViT-B) | KhÃ´ng cho sá»‘ quÃ¡ lá»›n trÆ°á»›c softmax |
| `softmax` | Chuyá»ƒn thÃ nh xÃ¡c suáº¥t (tá»•ng = 1) | [0.7, 0.2, 0.1] thay vÃ¬ [5.2, 1.3, 0.5] |
| `Ã— V` | Láº¥y ná»™i dung theo trá»ng sá»‘ | 70% tá»« sÃ¡ch 1, 20% tá»« sÃ¡ch 2, 10% tá»« sÃ¡ch 3 |

### Multi-Head Attention

Thay vÃ¬ 1 gÃ³c nhÃ¬n, dÃ¹ng nhiá»u gÃ³c nhÃ¬n (heads) cÃ¹ng lÃºc:

```
Head 1: Focus vÃ o texture (lÃ´ng, váº£y)
Head 2: Focus vÃ o shape (hÃ¬nh dÃ¡ng tá»•ng thá»ƒ)
Head 3: Focus vÃ o color (mÃ u sáº¯c)
...
Head 12: Focus vÃ o context (background)
```

ViT-B: 12 heads Ã— 64 chiá»u/head = 768 chiá»u tá»•ng

## 1.3 CLS Token

### Váº¥n Ä‘á»

Sau khi xá»­ lÃ½ 196 patches qua Transformer, ta cÃ³ 196 output vectors. NhÆ°ng ta cáº§n **1 vector duy nháº¥t** Ä‘áº¡i diá»‡n cho toÃ n bá»™ áº£nh.

### Giáº£i phÃ¡p: CLS Token

ThÃªm 1 token Ä‘áº·c biá»‡t á»Ÿ Ä‘áº§u sequence:

```
Input:  [CLS] [P1] [P2] [P3] ... [P196]
         â†“
    Transformer (12 layers)
         â†“
Output: [CLS'] [P1'] [P2'] [P3'] ... [P196']
         â†‘
    Láº¥y vector nÃ y lÃ m Ä‘áº¡i diá»‡n
```

**Táº¡i sao CLS hoáº¡t Ä‘á»™ng?**

CLS khÃ´ng gáº¯n vá»›i pixel nÃ o. Qua attention, nÃ³ "láº¯ng nghe" táº¥t cáº£ 196 patches vÃ  tá»± há»c cÃ¡ch tá»•ng há»£p thÃ´ng tin quan trá»ng nháº¥t.

```
CLS: "TÃ´i khÃ´ng biáº¿t gÃ¬, nhÆ°ng tÃ´i sáº½ há»i táº¥t cáº£ cÃ¡c patches"
     â†“
Layer 1: CLS attention Ä‘áº¿n P1, P2, P3...
Layer 2: CLS tinh chá»‰nh dá»±a trÃªn layer 1
...
Layer 12: CLS Ä‘Ã£ "hiá»ƒu" toÃ n bá»™ áº£nh
```

**Trong DINO**: Output CLS cá»§a Teacher vÃ  Student Ä‘Æ°á»£c so sÃ¡nh vá»›i nhau.

## 1.4 Knowledge Distillation

### Ã tÆ°á»Ÿng tá»« Hinton (2015)

Model lá»›n Ä‘Ã£ train xong (Teacher) â†’ "dáº¡y" cho model nhá» (Student)

**Táº¡i sao khÃ´ng chá»‰ dÃ¹ng nhÃ£n cá»©ng?**

```
NhÃ£n cá»©ng:  [1, 0, 0]  = "ÄÃ¢y lÃ  mÃ¨o"
NhÃ£n má»m:   [0.8, 0.15, 0.05] = "80% mÃ¨o, 15% chÃ³, 5% há»•"
```

NhÃ£n má»m chá»©a thÃ´ng tin há»¯u Ã­ch: "MÃ¨o giá»‘ng chÃ³ hÆ¡n giá»‘ng Ã´ tÃ´"

### Self-Distillation trong DINO

DINO Ä‘áº·c biá»‡t: Teacher vÃ  Student **cÃ¹ng kiáº¿n trÃºc, cÃ¹ng kÃ­ch thÆ°á»›c**. Teacher khÃ´ng train sáºµn mÃ  Ä‘Æ°á»£c táº¡o tá»« Student qua EMA.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Student   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ EMA (copy cháº­m)
                           â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Teacher   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 2: DINOv1 (2021)

## 2.1 Core Insight: Local â†’ Global

### VÃ­ dá»¥

Báº¡n tháº¥y má»™t cÃ¡i vÃ¢y cÃ¡. Báº¡n biáº¿t Ä‘Ã³ lÃ  con cÃ¡, dÃ¹ chá»‰ tháº¥y má»™t pháº§n nhá».

DINO Ã¡p dá»¥ng Ã½ tÆ°á»Ÿng nÃ y:
- **Teacher** nhÃ¬n toÃ n bá»™ áº£nh (global crop, >50% áº£nh)
- **Student** nhÃ¬n má»™t gÃ³c nhá» (local crop, <50% áº£nh)
- **Má»¥c tiÃªu**: Student pháº£i output giá»‘ng Teacher

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   â”‚     â”‚ Local   â”‚
â”‚   Global crop     â”‚     â”‚ crop    â”‚
â”‚   (Teacher nhÃ¬n)  â”‚     â”‚(Student)â”‚
â”‚                   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                      â†“
   Output: P_t              Output: P_s
        â†“                      â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€ Loss â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           P_s pháº£i â‰ˆ P_t
```

**Táº¡i sao Ä‘iá»u nÃ y hoáº¡t Ä‘á»™ng?**

Student chá»‰ tháº¥y cÃ¡i vÃ¢y, nhÆ°ng pháº£i Ä‘oÃ¡n "Ä‘Ã¢y lÃ  cÃ¡" (giá»‘ng Teacher Ä‘ang tháº¥y cáº£ con cÃ¡). Buá»™c Student pháº£i **hiá»ƒu ngá»¯ cáº£nh**, khÃ´ng chá»‰ copy pixel.

## 2.2 Kiáº¿n TrÃºc DINO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DINO Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  áº¢nh gá»‘c                                                     â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€â”€â†’ Global crops (2 cÃ¡i, 224Ã—224) â”€â”€â†’ Teacher ViT       â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                                     Projection Head     â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                                     softmax(Ï„=0.04)     â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                                         P_t             â”‚
â”‚    â”‚                                          â”‚              â”‚
â”‚    â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                      â–¼                                  â”‚
â”‚    â”‚              Loss = -Î£ P_t Â· log(P_s)                   â”‚
â”‚    â”‚                      â–²                                  â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                     P_s                                 â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                 softmax(Ï„=0.1)                          â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â”‚                Projection Head                          â”‚
â”‚    â”‚                      â”‚                                  â”‚
â”‚    â””â”€â”€â†’ Local crops (6 cÃ¡i, 96Ã—96) â”€â”€â”€â†’ Student ViT         â”‚
â”‚                                                              â”‚
â”‚    [EMA: Î¸_T â† 0.996Â·Î¸_T + 0.004Â·Î¸_S]                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡c thÃ nh pháº§n chÃ­nh

| ThÃ nh pháº§n | MÃ´ táº£ | Tham sá»‘ |
|------------|-------|---------|
| **ViT backbone** | Vision Transformer | ViT-B: 86M params |
| **Projection head** | MLP 3 layers | 768 â†’ 2048 â†’ 2048 â†’ K |
| **K (prototypes)** | Sá»‘ "categories" áº©n | 65,536 |
| **Ï„ (temperature)** | Äá»™ sáº¯c nÃ©t softmax | Teacher: 0.04, Student: 0.1 |
| **Î» (EMA)** | Tá»· lá»‡ giá»¯ Teacher | 0.996 â†’ 1.0 |

## 2.3 EMA: Exponential Moving Average

### CÃ´ng thá»©c

```
Î¸_T â† Î» Â· Î¸_T + (1-Î») Â· Î¸_S
```

Vá»›i Î» = 0.996:
- Teacher giá»¯ 99.6% giÃ¡ trá»‹ cÅ©
- Teacher láº¥y 0.4% tá»« Student

### Táº¡i sao cáº§n EMA?

**Váº¥n Ä‘á»**: Náº¿u cáº£ Teacher vÃ  Student cÃ¹ng train báº±ng gradient:
- Cáº£ hai Ä‘uá»•i theo nhau
- Cuá»‘i cÃ¹ng má»i áº£nh â†’ cÃ¹ng 1 output (collapse)

**Giáº£i phÃ¡p**: Teacher thay Ä‘á»•i cá»±c cháº­m
- Teacher á»•n Ä‘á»‹nh â†’ Student cÃ³ má»¥c tiÃªu rÃµ rÃ ng
- NhÆ° tháº§y giÃ¡o kinh nghiá»‡m: khÃ´ng Ä‘á»•i Ã½ kiáº¿n theo tá»«ng cÃ¢u há»i

### Cosine Schedule

Î» tÄƒng tá»« 0.996 â†’ 1.0 theo cosine schedule:
- Äáº§u training: Teacher há»c nhanh hÆ¡n (0.996)
- Cuá»‘i training: Teacher gáº§n nhÆ° khÃ´ng Ä‘á»•i (â‰ˆ1.0)

## 2.4 Multi-crop Strategy

```
Tá»« 1 áº£nh, táº¡o:
- 2 global crops: 224Ã—224, cover >50% áº£nh
- 6 local crops: 96Ã—96, cover <50% áº£nh

Teacher: chá»‰ nháº­n global crops
Student: nháº­n cáº£ global vÃ  local crops
```

**Chi phÃ­ tÃ­nh toÃ¡n**:
- Local crop 96Â² = 9,216 pixels
- Global crop 224Â² = 50,176 pixels
- 6 local â‰ˆ 1 global vá» compute

**Káº¿t quáº£**: 8 gÃ³c nhÃ¬n khÃ¡c nhau, chá»‰ +50% compute so vá»›i 2 crops.

## 2.5 Centering vÃ  Sharpening

### Collapse lÃ  gÃ¬?

Collapse = má»i áº£nh cho ra cÃ¹ng 1 output

CÃ³ 2 loáº¡i:
1. **Mode collapse**: Táº¥t cáº£ output = 1 vector cá»‘ Ä‘á»‹nh
2. **Uniform collapse**: Táº¥t cáº£ output = phÃ¢n bá»‘ Ä‘á»u [1/K, 1/K, ...]

### Centering (chá»‘ng mode collapse)

```
g(x) = f(x) - c
```

Trong Ä‘Ã³ c lÃ  running mean cá»§a output Teacher:
```
c â† m Â· c + (1-m) Â· mean(f(x))  (m = 0.9)
```

Trá»« Ä‘i mean â†’ khÃ´ng cho 1 chiá»u dominate.

### Sharpening (chá»‘ng uniform collapse)

Temperature Ï„ = 0.04 ráº¥t tháº¥p:
- Softmax output gáº§n one-hot
- Teacher buá»™c pháº£i "chá»n" rÃµ rÃ ng

```
KhÃ´ng sharpening: [0.2, 0.2, 0.2, 0.2, 0.2] â†’ Teacher nÃ³i "khÃ´ng biáº¿t"
CÃ³ sharpening:    [0.9, 0.03, 0.03, 0.02, 0.02] â†’ Teacher nÃ³i "Ä‘Ã¢y!"
```

### Ablation

| Setting | Káº¿t quáº£ |
|---------|---------|
| Bá» centering | Collapse sau 1 epoch |
| Bá» sharpening | Collapse cháº­m hÆ¡n, váº«n xáº£y ra |
| Cáº£ hai | á»”n Ä‘á»‹nh |

## 2.6 Emerging Properties

### Attention Map Tá»± Segment

PhÃ¡t hiá»‡n báº¥t ngá»: attention heads tá»± há»c cÃ¡ch tÃ¡ch váº­t thá»ƒ khá»i ná»n.

```
Head 1: Focus vÃ o Ä‘áº§u con chÃ³
Head 2: Focus vÃ o thÃ¢n con chÃ³
Head 3: Focus vÃ o ná»n (cá», trá»i)
```

**Táº¡i sao xáº£y ra?**
- Supervised: Chá»‰ cáº§n biáº¿t "Ä‘Ã¢y lÃ  chÃ³" â†’ khÃ´ng cáº§n hiá»ƒu Ä‘Ã¢u lÃ  Ä‘áº§u
- DINO: So sÃ¡nh local/global crop â†’ buá»™c pháº£i hiá»ƒu vÃ¹ng nÃ o quan trá»ng

### Káº¿t quáº£ v1

| Model | ImageNet Linear Probe |
|-------|----------------------|
| ViT-S/8 | 79.7% |
| ViT-B/8 | **80.1%** |
| ResNet-50 (DINO) | 75.3% |
| ResNet-50 (Supervised) | 76.5% |

**Milestone**: Láº§n Ä‘áº§u self-supervised vÆ°á»£t supervised!

---

# Part 3: DINOv2 (2023)

## 3.1 Motivation: 4 Háº¡n Cháº¿ cá»§a v1

| Háº¡n cháº¿ v1 | Giáº£i phÃ¡p v2 |
|------------|--------------|
| Data nhá» (1.28M) | LVD-142M (Ã—110) |
| Model nhá» (86M) | ViT-g (1.1B, Ã—13) |
| Chá»‰ classification | Dense tasks (seg, depth) |
| Chá»‰ DINO loss | DINO + iBOT + KoLeo |

## 3.2 LVD-142M: Data Curation

### Pipeline

```
Crawl 1.2B áº£nh tá»« internet
         â”‚
         â–¼
    Lá»c NSFW, low-quality
         â”‚
         â–¼
    Loáº¡i trÃ¹ng báº±ng SSCD
    (Self-Supervised Copy Detection)
         â”‚
         â–¼
    Chá»n qua Faiss nearest neighbor
    (embed áº£nh, so vá»›i ImageNet embedding)
         â”‚
         â–¼
    142M áº£nh curated
```

### Key insight: Quality > Quantity

| Data | ImageNet Linear |
|------|-----------------|
| Raw 1.2B | 84.2% |
| Curated 142M | **86.5%** |

Ãt hÆ¡n 8 láº§n nhÆ°ng káº¿t quáº£ tá»‘t hÆ¡n!

**Táº¡i sao?**
- Raw data: trÃ¹ng láº·p, NSFW, logo, watermark, phÃ¢n bá»‘ lá»‡ch
- Curated: Ä‘a dáº¡ng, sáº¡ch, cÃ¢n báº±ng

## 3.3 Three Losses

### DINO Loss (káº¿ thá»«a tá»« v1)

```
L_DINO = -Î£ P_t Â· log(P_s)
```

DÃ¹ng CLS token â†’ hiá»ƒu toÃ n cá»¥c.

### iBOT Loss (má»›i)

**Ã tÆ°á»Ÿng**: Che má»™t sá»‘ patches, báº¯t Student Ä‘oÃ¡n token tá»« Teacher.

```
Input:     [CLS] [P1] [MASK] [P3] [MASK] [P5] ...
                       â†“           â†“
                  ÄoÃ¡n token    ÄoÃ¡n token
                  tá»« Teacher    tá»« Teacher
```

**KhÃ¡c MAE tháº¿ nÃ o?**

| Aspect | MAE | iBOT |
|--------|-----|------|
| ÄoÃ¡n gÃ¬? | Pixel RGB | Semantic token |
| Level | Low-level | High-level |
| VÃ­ dá»¥ | "Pixel mÃ u xanh" | "ÄÃ¢y lÃ  pháº§n tai" |

**Táº¡i sao quan trá»ng?**
- DINO loss: Chá»‰ hiá»ƒu global (CLS)
- iBOT loss: Hiá»ƒu local (tá»«ng patch)
- Cáº§n cáº£ hai cho dense tasks (segmentation)

### KoLeo Loss (má»›i)

**Váº¥n Ä‘á»**: Embeddings cÃ³ thá»ƒ tá»¥ láº¡i thÃ nh clusters â†’ máº¥t Ä‘a dáº¡ng.

**Giáº£i phÃ¡p**: Äáº©y nearest neighbors ra xa.

```
L_KoLeo = -1/n Î£áµ¢ log(min_{jâ‰ i} ||záµ¢ - zâ±¼||)
```

**Trá»±c quan**:
```
Before KoLeo:          After KoLeo:
    â—â—â—                    â—      â—
   â—â—â—â—â—      â†’              â—  â—
    â—â—â—                  â—      â—
(tá»¥ láº¡i)              (tráº£i Ä‘á»u)
```

### Ablation

| Bá» loss nÃ o? | ImageNet | ADE20k (seg) |
|--------------|----------|--------------|
| Baseline | 86.5% | 49.0 |
| Bá» iBOT | 86.3% | **44.8** (âˆ’4.2) |
| Bá» KoLeo | 86.0% | 48.5 |

iBOT quan trá»ng nháº¥t cho dense tasks!

## 3.4 Register Tokens

### Váº¥n Ä‘á»: Attention Artifacts

Khi scale ViT lÃªn lá»›n + high resolution:
- Má»™t sá»‘ positions nháº­n attention khÃ´ng há»£p lÃ½
- Attention map cÃ³ "vÃ¹ng cháº¿t"

### Giáº£i phÃ¡p

ThÃªm 4-8 learnable tokens (khÃ´ng gáº¯n vá»›i patch nÃ o):

```
[CLS] [REG1] [REG2] [REG3] [REG4] [P1] [P2] ... [P196]
       â†‘      â†‘      â†‘      â†‘
    "Parking spots" cho extra attention
```

Register tokens Ä‘Ã³ng vai trÃ² "bÃ£i Ä‘á»—" â€” hÃºt attention thá»«a, lÃ m sáº¡ch attention map cho cÃ¡c patches tháº­t.

## 3.5 Káº¿t Quáº£ v2

| Benchmark | DINOv2 | iBOT | MAE | OpenCLIP |
|-----------|--------|------|-----|----------|
| ImageNet | **86.5%** | 82.3% | 73.5% | 83.5% |
| ADE20k | **49.0** | 44.8 | â€” | â€” |

**DINOv2 = Foundation model cho vision**
- 1 backbone dÃ¹ng cho nhiá»u tasks
- KhÃ´ng cáº§n fine-tune (frozen features)
- VÆ°á»£t táº¥t cáº£ phÆ°Æ¡ng phÃ¡p khÃ¡c

---

# Part 4: DINOv3 (2025)

## 4.1 Scaling Challenges

### CÃ¢u há»i: Scale cÃ³ giá»›i háº¡n khÃ´ng?

| Aspect | v2 | v3 | Scale |
|--------|-----|-----|-------|
| Model | 1.1B | 7B | Ã—6.4 |
| Data | 142M | 1.69B | Ã—12 |

### Váº¥n Ä‘á»: Training Instability

Model 7B params khÃ´ng dá»… train:
- Loss bÃ¹ng ná»• (explode)
- Gradient khÃ´ng á»•n Ä‘á»‹nh
- Diverge sau vÃ i nghÃ¬n steps

## 4.2 Gram Anchoring

### Gram Matrix lÃ  gÃ¬?

Cho feature matrix F (má»—i cá»™t lÃ  1 feature):

```
G = F Ã— Fáµ€
```

G[i,j] = dot product giá»¯a feature i vÃ  feature j = **correlation**

```
Feature matrix F:          Gram matrix G:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ f1  f2  f3  f4  â”‚        â”‚ f1Â·f1  f1Â·f2 ...â”‚
â”‚ â”‚   â”‚   â”‚   â”‚   â”‚   â†’    â”‚ f2Â·f1  f2Â·f2 ...â”‚
â”‚ â”‚   â”‚   â”‚   â”‚   â”‚        â”‚ ...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Táº¡i sao Gram Anchoring á»•n Ä‘á»‹nh training?

**Váº¥n Ä‘á»**: Model lá»›n â†’ features thay Ä‘á»•i nhanh â†’ correlation thay Ä‘á»•i nhanh â†’ unstable

**Giáº£i phÃ¡p**: Enforce G_student â‰ˆ G_teacher

```
L_Gram = ||G_student - G_teacher||Â²
```

Giá»¯ cáº¥u trÃºc correlation á»•n Ä‘á»‹nh, features khÃ´ng "cháº¡y lung tung".

### Ablation

| Setting | Káº¿t quáº£ |
|---------|---------|
| Without Gram Anchoring | Diverge at ~5K steps |
| With Gram Anchoring | Stable to 1M+ steps |

## 4.3 Text Alignment vs CLIP

### So sÃ¡nh hai approaches

| Aspect | CLIP | DINOv3 |
|--------|------|--------|
| Training | Joint (vision+text cÃ¹ng lÃºc) | Decoupled (vision trÆ°á»›c, text sau) |
| Data | 400M image-text pairs | Images only, text optional |
| Vision bias | Bias vá» text | Pure visual |
| Khi thÃªm text | â€” | KhÃ´ng lÃ m há»ng vision |

### DINOv3 Strategy

```
Phase 1: Train vision encoder (DINO)
              â†“
Phase 2: Freeze vision, train text alignment
              â†“
         Shared embedding space
```

**Æ¯u Ä‘iá»ƒm**:
- Vision features há»c pure visual understanding
- ThÃªm text khÃ´ng áº£nh hÆ°á»Ÿng vision performance
- "Learn to see first, learn to talk later"

## 4.4 When Does Scaling Help?

### Káº¿t quáº£

| Benchmark | v2 | v3 | Î” |
|-----------|-----|-----|-----|
| ImageNet | 86.5% | 88.4% | +1.9 |
| ADE20k | 49.0 | 55.9 | **+6.9** |
| DAVIS | 76.6 | 83.3 | **+6.7** |

### Key Insight

**Dense tasks Ä‘Æ°á»£c lá»£i nhiá»u nháº¥t tá»« scaling!**

- Classification: Gáº§n bÃ£o hÃ²a (88% â†’ khÃ³ tÄƒng thÃªm)
- Segmentation: Cáº§n hiá»ƒu tá»«ng pixel â†’ scale giÃºp nhiá»u
- Video: Temporal understanding â†’ scale giÃºp nhiá»u

### HÆ°á»›ng Ä‘i tÆ°Æ¡ng lai

KhÃ´ng cáº§n scale classification ná»¯a. Focus vÃ o:
- Dense prediction (segmentation, depth)
- Video understanding
- 3D vision
- Multimodal

---

# Appendices

## A. Vietnamese Glossary

| English | Vietnamese | Äá»‹nh nghÄ©a |
|---------|------------|------------|
| Attention | CÆ¡ cháº¿ chÃº Ã½ | CÃ¡ch model quyáº¿t Ä‘á»‹nh nÃªn "nhÃ¬n" vÃ o Ä‘Ã¢u |
| Attention head | Äáº§u attention | Má»™t gÃ³c nhÃ¬n Ä‘á»™c láº­p trong multi-head attention |
| CLS token | Token phÃ¢n loáº¡i | Vector Ä‘áº¡i diá»‡n cho toÃ n bá»™ áº£nh |
| Collapse | Sá»¥p Ä‘á»• | Lá»—i khi má»i áº£nh cho ra cÃ¹ng output |
| Contrastive | TÆ°Æ¡ng pháº£n | PhÆ°Æ¡ng phÃ¡p kÃ©o giá»‘ng gáº§n, Ä‘áº©y khÃ¡c xa |
| Cross-entropy | Entropy chÃ©o | Loss Ä‘o sá»± khÃ¡c biá»‡t giá»¯a 2 phÃ¢n bá»‘ |
| Dense prediction | Dá»± Ä‘oÃ¡n dÃ y | Dá»± Ä‘oÃ¡n cho tá»«ng pixel (segmentation, depth) |
| Distillation | ChÆ°ng cáº¥t | Chuyá»ƒn tri thá»©c tá»« model lá»›n sang nhá» |
| Embedding | NhÃºng / Vector Ä‘áº·c trÆ°ng | Biá»ƒu diá»…n dá»¯ liá»‡u dÆ°á»›i dáº¡ng vector sá»‘ |
| EMA | Trung bÃ¬nh Ä‘á»™ng | CÃ¡ch cáº­p nháº­t Teacher tá»« Student |
| Foundation model | Model ná»n táº£ng | Model pretrained dÃ¹ng cho nhiá»u tasks |
| Gram matrix | Ma tráº­n Gram | FFáµ€, Ä‘o tÆ°Æ¡ng quan giá»¯a features |
| iBOT | iBOT | Masked prediction á»Ÿ má»©c semantic |
| KoLeo | KoLeo | Regularization Ä‘áº©y embeddings tráº£i Ä‘á»u |
| Linear probe | Äáº§u tuyáº¿n tÃ­nh | Chá»‰ train 1 layer linear trÃªn frozen features |
| mIoU | mIoU | Mean Intersection over Union (Ä‘o segmentation) |
| Multi-crop | Cáº¯t nhiá»u gÃ³c | Táº¡o nhiá»u views tá»« 1 áº£nh |
| Patch | Máº£nh | Má»™t pháº§n nhá» cá»§a áº£nh (16Ã—16 pixels) |
| Positional encoding | MÃ£ hÃ³a vá»‹ trÃ­ | Cho model biáº¿t patch á»Ÿ Ä‘Ã¢u |
| Projection head | Äáº§u chiáº¿u | MLP sau ViT, chuyá»ƒn Ä‘á»•i features |
| Register token | Token Ä‘Äƒng kÃ½ | Learnable tokens Ä‘á»ƒ "hÃºt" extra attention |
| Self-supervised | Tá»± giÃ¡m sÃ¡t | Há»c khÃ´ng cáº§n nhÃ£n ngÆ°á»i gÃ¡n |
| Sharpening | LÃ m sáº¯c | DÃ¹ng temperature tháº¥p cho phÃ¢n bá»‘ sáº¯c nÃ©t |
| Softmax | Softmax | Chuyá»ƒn sá»‘ thÃ´ thÃ nh xÃ¡c suáº¥t (tá»•ng = 1) |
| Temperature | Nhiá»‡t Ä‘á»™ | Kiá»ƒm soÃ¡t Ä‘á»™ sáº¯c/má»m cá»§a phÃ¢n bá»‘ |
| Transformer | Transformer | Kiáº¿n trÃºc dÃ¹ng attention |
| ViT | Vision Transformer | Transformer cho áº£nh |

## B. Math Notation Reference

| Notation | Meaning |
|----------|---------|
| Î¸_T, Î¸_S | Parameters cá»§a Teacher, Student |
| P_t, P_s | Output probability cá»§a Teacher, Student |
| Ï„ | Temperature |
| Î» | EMA coefficient |
| K | Sá»‘ prototypes (65,536 trong DINO) |
| d | Dimension (768 trong ViT-B) |
| Q, K, V | Query, Key, Value trong attention |
| G | Gram matrix |
| L | Loss function |

## C. Further Reading

### Papers
1. Caron et al. ICCV 2021 â€” DINOv1
2. Oquab et al. TMLR 2024 â€” DINOv2
3. SimÃ©oni et al. arXiv 2025 â€” DINOv3
4. Dosovitskiy et al. ICLR 2021 â€” ViT
5. Hinton et al. arXiv 2015 â€” Knowledge Distillation
6. Zhou et al. ICLR 2022 â€” iBOT
7. Darcet et al. ICLR 2024 â€” Register Tokens

### Code
- DINOv2: https://github.com/facebookresearch/dinov2
- ViT: https://github.com/google-research/vision_transformer

---

*TÃ i liá»‡u nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ cho CS students cÃ³ ML cÆ¡ báº£n. Má»i concept Ä‘á»u cÃ³ vÃ­ dá»¥ trÆ°á»›c, lÃ½ thuyáº¿t sau.*
